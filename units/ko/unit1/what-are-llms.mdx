# LLM이란 무엇인가요?

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-1.jpg" alt="유닛 1 계획"/>

이전 섹션에서 우리는 각 에이전트의 핵심에는 **AI 모델**이 필요하며, LLM이 이 목적에 가장 널리 사용되는 AI 모델임을 배웠습니다.

이제 LLM이 무엇이며 어떻게 에이전트를 구동하는지 알아보겠습니다.

이 섹션에서는 LLM의 활용에 대한 간결한 기술적 설명을 제공합니다. 더 깊이 배우고 싶다면 <a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank">무료 자연어처리(NLP) 강의</a>를 참고하세요.

## 대형 언어 모델(LLM)이란?

LLM은 **인간 언어를 이해하고 생성하는 데 뛰어난** AI 모델입니다. 방대한 텍스트 데이터로 학습되어 언어의 패턴, 구조, 뉘앙스까지 익힙니다. 이 모델들은 수백만~수십억 개의 파라미터로 구성됩니다.

대부분의 LLM은 **트랜스포머(Transformer) 아키텍처**를 기반으로 합니다. 이 구조는 2018년 Google의 BERT 발표 이후 큰 주목을 받았으며, "어텐션(Attention)" 알고리즘에 기반한 딥러닝 구조입니다.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/transformer.jpg" alt="트랜스포머"/>
<figcaption>원래의 트랜스포머 구조는 왼쪽에 인코더, 오른쪽에 디코더가 있습니다.</figcaption>
</figure>

트랜스포머에는 3가지 유형이 있습니다:

1. **인코더(Encoder)**  
   인코더 기반 트랜스포머는 텍스트(또는 기타 데이터)를 입력받아 그 텍스트의 밀집 표현(임베딩)을 출력합니다.

   - **예시**: Google의 BERT
   - **사용 사례**: 텍스트 분류, 의미 검색, 개체명 인식(NER)
   - **일반 크기**: 수백만 개의 파라미터

2. **디코더(Decoder)**  
   디코더 기반 트랜스포머는 **시퀀스를 완성하기 위해 새로운 토큰을 한 번에 하나씩 생성**하는 데 집중합니다.

   - **예시**: Meta의 Llama
   - **사용 사례**: 텍스트 생성, 챗봇, 코드 생성
   - **일반 크기**: 수십억(10^9) 개의 파라미터

3. **시퀀스-투-시퀀스(Seq2Seq, 인코더–디코더)**  
   시퀀스-투-시퀀스 트랜스포머는 인코더와 디코더를 결합합니다. 인코더가 입력 시퀀스를 컨텍스트 표현으로 처리한 뒤, 디코더가 출력 시퀀스를 생성합니다.

   - **예시**: T5, BART
   - **사용 사례**: 번역, 요약, 패러프레이징
   - **일반 크기**: 수백만 개의 파라미터

대형 언어 모델은 다양한 형태로 존재하지만, LLM은 일반적으로 수십억 개의 파라미터를 가진 디코더 기반 모델입니다. 대표적인 LLM은 다음과 같습니다:

| **모델**         | **제공자**                |
|----------------|--------------------------|
| **Deepseek-R1** | DeepSeek                 |
| **GPT4**        | OpenAI                   |
| **Llama 3**     | Meta (Facebook AI Research) |
| **SmolLM2**     | Hugging Face             |
| **Gemma**       | Google                   |
| **Mistral**     | Mistral                  |

LLM의 기본 원리는 단순하지만 매우 강력합니다: **이전 토큰 시퀀스가 주어졌을 때, 다음 토큰을 예측하는 것**이 목표입니다. "토큰"은 LLM이 다루는 정보의 단위입니다. 토큰을 "단어"와 비슷하게 생각할 수 있지만, 효율성을 위해 LLM은 전체 단어가 아닌 부분 단위(서브워드)를 사용합니다.

예를 들어, 영어에는 약 60만 개의 단어가 있지만, LLM의 어휘는 약 32,000개(예: Llama 2) 정도입니다. 토크나이저는 종종 결합 가능한 서브워드 단위로 동작합니다.

예를 들어, "interest"와 "ing" 토큰을 결합해 "interesting"을 만들거나, "ed"를 붙여 "interested"를 만들 수 있습니다.

아래 인터랙티브 플레이그라운드에서 다양한 토크나이저를 실험해볼 수 있습니다:

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

각 LLM은 모델별로 **특수 토큰**을 가지고 있습니다. LLM은 이 토큰을 사용해 생성의 구조적 요소(시퀀스, 메시지, 응답 등)의 시작과 끝을 표시합니다. 우리가 모델에 입력하는 프롬프트도 특수 토큰으로 구조화됩니다. 그중 가장 중요한 것이 **시퀀스 종료 토큰(EOS)** 입니다.

특수 토큰의 형태는 모델 제공자마다 매우 다양합니다.

아래 표는 특수 토큰의 다양성을 보여줍니다.

<table>
  <thead>
    <tr>
      <th><strong>모델</strong></th>
      <th><strong>제공자</strong></th>
      <th><strong>EOS 토큰</strong></th>
      <th><strong>기능</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT4</strong></td>
      <td>OpenAI</td>
      <td><code>&lt;|endoftext|&gt;</code></td>
      <td>메시지 텍스트 종료</td>
    </tr>
    <tr>
      <td><strong>Llama 3</strong></td>
      <td>Meta (Facebook AI Research)</td>
      <td><code>&lt;|eot_id|&gt;</code></td>
      <td>시퀀스 종료</td>
    </tr>
    <tr>
      <td><strong>Deepseek-R1</strong></td>
      <td>DeepSeek</td>
      <td><code>&lt;|end_of_sentence|&gt;</code></td>
      <td>메시지 텍스트 종료</td>
    </tr>
    <tr>
      <td><strong>SmolLM2</strong></td>
      <td>Hugging Face</td>
      <td><code>&lt;|im_end|&gt;</code></td>
      <td>지시문 또는 메시지 종료</td>
    </tr>
    <tr>
      <td><strong>Gemma</strong></td>
      <td>Google</td>
      <td><code>&lt;end_of_turn&gt;</code></td>
      <td>대화 턴 종료</td>
    </tr>
  </tbody>
</table>

<Tip>

이러한 특수 토큰을 외울 필요는 없지만, 그 다양성과 LLM의 텍스트 생성에서의 역할을 이해하는 것이 중요합니다. 더 알고 싶다면 각 모델의 Hub 저장소에서 설정 파일을 확인할 수 있습니다. 예를 들어, SmolLM2 모델의 특수 토큰은 <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/main/tokenizer_config.json">tokenizer_config.json</a>에서 볼 수 있습니다.

</Tip>

## 다음 토큰 예측 이해하기

LLM은 **오토리그레시브(autoregressive)** 모델로, **한 번의 출력이 다음 입력이 되는** 구조입니다. 이 루프는 모델이 EOS 토큰을 예측할 때까지 반복됩니다.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif" alt="오토리그레시브 디코딩 시각화" width="60%">

즉, LLM은 EOS에 도달할 때까지 텍스트를 디코딩합니다. 그렇다면 한 번의 디코딩 루프에서 무슨 일이 일어날까요?

전체 과정은 다소 복잡하지만, 에이전트 학습 목적에 맞게 간단히 요약하면:

- 입력 텍스트가 **토크나이즈**되면, 모델은 각 토큰의 의미와 위치 정보를 담은 시퀀스 표현을 계산합니다.
- 이 표현이 모델에 입력되어, 어휘 내 각 토큰이 다음에 올 확률을 점수로 출력합니다.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif" alt="디코딩 시각화" width="60%">

이 점수를 바탕으로 문장을 완성할 토큰을 선택하는 다양한 전략이 있습니다.

- 가장 간단한 디코딩 전략은 항상 점수가 가장 높은 토큰을 선택하는 것입니다.

아래 Space에서 SmolLM2로 디코딩 과정을 직접 체험해볼 수 있습니다(이 모델의 **EOS** 토큰은 **<|im_end|>** 입니다):

<iframe
	src="https://agents-course-decoding-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

- 더 발전된 디코딩 전략도 있습니다. 예를 들어, *빔 서치(beam search)*는 여러 후보 시퀀스를 탐색해 전체 점수가 가장 높은 시퀀스를 찾습니다(개별 토큰 점수는 낮을 수 있음).

<iframe
	src="https://agents-course-beam-search-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

디코딩에 대해 더 알고 싶다면 [NLP 강의](https://huggingface.co/learn/nlp-course)를 참고하세요.

## 어텐션이 전부다(Attention is all you need)

트랜스포머 아키텍처의 핵심은 **어텐션(Attention)** 입니다. 다음 단어를 예측할 때, 문장의 모든 단어가 똑같이 중요한 것은 아닙니다. 예를 들어, "The capital of France is ..."에서 "France"와 "capital"이 가장 중요한 의미를 가집니다.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="어텐션 시각화" width="60%">
이렇게 다음 토큰을 예측하는 데 가장 관련 있는 단어를 찾는 과정이 매우 효과적임이 입증되었습니다.

LLM의 기본 원리(다음 토큰 예측)는 GPT-2 이후로 변하지 않았지만, 신경망의 규모를 키우고 어텐션 메커니즘이 더 긴 시퀀스에도 잘 작동하도록 하는 데 큰 발전이 있었습니다.

LLM을 사용해본 적이 있다면 *컨텍스트 길이(context length)*라는 용어에 익숙할 것입니다. 이는 LLM이 한 번에 처리할 수 있는 최대 토큰 수, 즉 최대 어텐션 범위를 의미합니다.

## LLM 프롬프트 설계의 중요성

LLM의 유일한 임무가 모든 입력 토큰을 보고 다음 토큰을 예측하는 것, 그리고 어떤 토큰이 중요한지 선택하는 것임을 고려하면, 입력 시퀀스(프롬프트)의 문구가 매우 중요합니다.

LLM에 제공하는 입력 시퀀스를 _프롬프트(prompt)_라고 합니다. 프롬프트를 신중하게 설계하면 **원하는 출력을 얻도록 LLM의 생성을 유도**할 수 있습니다.

## LLM은 어떻게 학습되나요?

LLM은 대규모 텍스트 데이터셋에서, 시퀀스 내 다음 단어를 예측하는 자기지도(self-supervised) 또는 마스킹 언어 모델링 방식으로 학습됩니다.

이러한 비지도 학습을 통해 모델은 언어의 구조와 **텍스트 내의 패턴**을 익혀, 보지 못한 데이터에도 일반화할 수 있습니다.

이 초기 _사전학습(pre-training)_ 이후, LLM은 감독 학습(supervised learning)으로 특정 작업에 맞게 파인튜닝될 수 있습니다. 예를 들어, 일부 모델은 대화 구조나 도구 사용에 맞게, 다른 모델은 분류나 코드 생성에 맞게 추가 학습됩니다.

## LLM을 어떻게 사용할 수 있나요?

두 가지 주요 방법이 있습니다:

1. **로컬에서 실행** (충분한 하드웨어가 있다면)
2. **클라우드/API 사용** (예: Hugging Face Serverless Inference API)

이 강의에서는 주로 Hugging Face Hub의 API를 통해 모델을 사용할 것입니다. 이후에는 여러분의 하드웨어에서 직접 모델을 실행하는 방법도 다룹니다.

## LLM은 AI 에이전트에서 어떻게 사용되나요?

LLM은 AI 에이전트의 핵심 구성 요소로, **인간 언어를 이해하고 생성하는 기반**을 제공합니다.

사용자 지시를 해석하고, 대화의 맥락을 유지하며, 계획을 세우고 사용할 도구를 결정할 수 있습니다.

이 단원에서 이러한 단계를 더 자세히 다룰 예정이지만, 지금은 LLM이 **에이전트의 두뇌**라는 점을 이해하면 충분합니다.

---

정보가 많았죠! 이제 LLM이 무엇이고, 어떻게 작동하며, AI 에이전트에 어떤 역할을 하는지 기본을 익혔습니다.

언어 모델과 자연어처리의 더 깊은 세계가 궁금하다면 <a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank">무료 NLP 강의</a>를 꼭 확인해보세요.

이제 LLM이 어떻게 대화형 맥락에서 출력을 구조화하는지 살펴볼 차례입니다.

<a href="https://huggingface.co/agents-course/notebooks/blob/main/unit1/dummy_agent_library.ipynb" target="_blank">이 노트북</a>을 실행하려면 <a href="https://hf.co/settings/tokens" target="_blank">https://hf.co/settings/tokens</a>에서 받을 수 있는 Hugging Face 토큰이 필요합니다.

Jupyter 노트북 실행법은 <a href="https://huggingface.co/docs/hub/notebooks">Hugging Face Hub의 Jupyter Notebooks 안내</a>를 참고하세요.

또한 <a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" target="_blank">Meta Llama 모델</a>에 대한 액세스 요청도 필요합니다.