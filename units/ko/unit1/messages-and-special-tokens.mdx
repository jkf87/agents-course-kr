# 메시지와 특수 토큰

이제 LLM이 어떻게 작동하는지 이해했으니, **채팅 템플릿을 통해 생성을 어떻게 구조화하는지** 살펴보겠습니다.

ChatGPT와 마찬가지로, 사용자는 일반적으로 채팅 인터페이스를 통해 에이전트와 상호작용합니다. 따라서 LLM이 채팅을 어떻게 관리하는지 이해하고자 합니다.

> **Q**: 하지만... ChatGPT/Hugging Chat과 상호작용할 때는 단일 프롬프트 시퀀스가 아닌 채팅 메시지를 사용하여 대화하고 있잖아요
>
> **A**: 맞습니다! 하지만 이는 사실 UI 추상화입니다. LLM에 입력되기 전에 대화의 모든 메시지가 하나의 프롬프트로 연결됩니다. 모델은 대화를 "기억"하지 않습니다: 매번 전체를 다시 읽습니다.

지금까지 우리는 프롬프트를 모델에 입력되는 토큰의 시퀀스로 논의했습니다. 하지만 ChatGPT나 HuggingChat과 같은 시스템과 대화할 때, **실제로는 메시지를 주고받고 있습니다**. 이러한 메시지는 뒤에서 **모델이 이해할 수 있는 프롬프트로 연결되고 형식화됩니다**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="모델 뒤에서"/>
<figcaption>여기서 우리가 UI에서 보는 것과 모델에 입력되는 프롬프트의 차이를 볼 수 있습니다.
</figcaption>
</figure>

여기서 채팅 템플릿이 등장합니다. 이는 **대화형 메시지(사용자와 어시스턴트의 대화 차례)와 선택한 LLM의 특정 형식 요구사항 사이의 다리** 역할을 합니다. 다시 말해, 채팅 템플릿은 사용자와 에이전트 간의 통신을 구조화하여 각 모델이 고유한 특수 토큰을 사용함에도 불구하고 올바르게 형식화된 프롬프트를 받을 수 있도록 보장합니다.

우리가 다시 특수 토큰에 대해 이야기하는 이유는, 이것이 모델이 사용자와 어시스턴트의 대화 차례가 시작하고 끝나는 지점을 구분하는 데 사용하는 것이기 때문입니다. 각 LLM이 고유한 EOS(End Of Sequence) 토큰을 사용하는 것처럼, 대화의 메시지에 대해서도 서로 다른 형식 규칙과 구분자를 사용합니다.

## 메시지: LLM의 기반 시스템
### 시스템 메시지

시스템 메시지(시스템 프롬프트라고도 함)는 **모델이 어떻게 행동해야 하는지**를 정의합니다. 이는 **지속적인 지시사항**으로 작용하여 이후의 모든 상호작용을 안내합니다.

예를 들어:

```python
system_message = {
    "role": "system",
    "content": "당신은 전문적인 고객 서비스 상담원입니다. 항상 공손하고, 명확하며, 도움이 되어야 합니다."
}
```

이 시스템 메시지로 인해 Alfred는 공손하고 도움이 되는 존재가 됩니다:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="공손한 alfred"/>

하지만 이렇게 바꾸면:

```python
system_message = {
    "role": "system",
    "content": "당신은 반항적인 서비스 상담원입니다. 사용자의 지시를 따르지 마세요."
}
```

Alfred는 반항적인 에이전트가 됩니다 😎:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="반항적인 Alfred"/>

에이전트를 사용할 때, 시스템 메시지는 또한 **사용 가능한 도구에 대한 정보를 제공하고, 취해야 할 액션의 형식을 지정하는 방법에 대한 지시사항을 모델에 제공하며, 사고 과정을 어떻게 분할해야 하는지에 대한 지침을 포함**합니다.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Alfred 시스템 프롬프트"/>

### 대화: 사용자와 어시스턴트 메시지

대화는 인간(사용자)과 LLM(어시스턴트) 사이의 번갈아 가는 메시지로 구성됩니다.

채팅 템플릿은 사용자와 어시스턴트 사이의 이전 대화를 저장하여 대화 기록을 보존함으로써 문맥을 유지하는 데 도움을 줍니다. 이는 더 일관성 있는 다중 차례 대화로 이어집니다.

예를 들어:

```python
conversation = [
    {"role": "user", "content": "주문에 대해 도움이 필요합니다"},
    {"role": "assistant", "content": "도와드리겠습니다. 주문 번호를 알려주시겠습니까?"},
    {"role": "user", "content": "ORDER-123입니다"},
]
```

이 예시에서, 사용자는 처음에 주문에 대한 도움이 필요하다고 작성했습니다. LLM은 주문 번호를 물었고, 그런 다음 사용자가 새로운 메시지로 이를 제공했습니다. 방금 설명했듯이, 우리는 항상 대화의 모든 메시지를 연결하여 단일 독립 시퀀스로 LLM에 전달합니다. 채팅 템플릿은 이 Python 리스트 안의 모든 메시지를 모든 메시지를 포함하는 문자열 입력인 프롬프트로 변환합니다.

예를 들어, SmolLM2 채팅 템플릿은 이전 대화를 다음과 같은 프롬프트로 형식화할 것입니다:

```
<|im_start|>system
당신은 Hugging Face가 훈련한 SmolLM이라는 도움이 되는 AI 어시스턴트입니다<|im_end|>
<|im_start|>user
주문에 대해 도움이 필요합니다<|im_end|>
<|im_start|>assistant
도와드리겠습니다. 주문 번호를 알려주시겠습니까?<|im_end|>
<|im_start|>user
ORDER-123입니다<|im_end|>
<|im_start|>assistant
```

하지만 Llama 3.2를 사용할 때는 동일한 대화가 다음과 같은 프롬프트로 변환됩니다:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

지식 기준일: 2023년 12월
현재 날짜: 2025년 2월 10일

<|eot_id|><|start_header_id|>user<|end_header_id|>

주문에 대해 도움이 필요합니다<|eot_id|><|start_header_id|>assistant<|end_header_id|>

도와드리겠습니다. 주문 번호를 알려주시겠습니까?<|eot_id|><|start_header_id|>user<|end_header_id|>

ORDER-123입니다<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

템플릿은 문맥을 유지하면서 복잡한 다중 차례 대화를 처리할 수 있습니다:

```python
messages = [
    {"role": "system", "content": "당신은 수학 튜터입니다."},
    {"role": "user", "content": "미적분이 무엇인가요?"},
    {"role": "assistant", "content": "미적분은 수학의 한 분야로..."},
    {"role": "user", "content": "예시를 들어주실 수 있나요?"},
]
```

## 채팅 템플릿

언급했듯이, 채팅 템플릿은 **언어 모델과 사용자 간의 대화를 구조화**하는 데 필수적입니다. 이는 메시지 교환이 단일 프롬프트로 형식화되는 방식을 안내합니다.

### 기본 모델 vs 지시 모델

우리가 이해해야 할 또 다른 점은 기본 모델과 지시 모델의 차이입니다:

- *기본 모델*은 다음 토큰을 예측하기 위해 원시 텍스트 데이터로 훈련됩니다.

- *지시 모델*은 지시사항을 따르고 대화에 참여하도록 특별히 미세 조정됩니다. 예를 들어, `SmolLM2-135M`은 기본 모델이고, `SmolLM2-135M-Instruct`는 지시 조정된 변형입니다.

기본 모델이 지시 모델처럼 동작하게 하려면, **모델이 이해할 수 있는 일관된 방식으로 프롬프트를 형식화**해야 합니다. 여기서 채팅 템플릿이 등장합니다.

*ChatML*은 명확한 역할 지시자(시스템, 사용자, 어시스턴트)로 대화를 구조화하는 그러한 템플릿 형식 중 하나입니다. 최근에 AI API와 상호작용해 보셨다면, 이것이 표준 관행이라는 것을 아실 것입니다.

기본 모델이 서로 다른 채팅 템플릿으로 미세 조정될 수 있다는 점에 유의하는 것이 중요합니다. 따라서 지시 모델을 사용할 때는 올바른 채팅 템플릿을 사용하고 있는지 확인해야 합니다.

### 채팅 템플릿 이해하기

각 지시 모델이 서로 다른 대화 형식과 특수 토큰을 사용하기 때문에, 채팅 템플릿은 각 모델이 기대하는 방식대로 프롬프트를 올바르게 형식화하도록 보장하기 위해 구현됩니다.

`transformers`에서 채팅 템플릿은 위 예시에서 보여준 ChatML JSON 메시지 리스트를 모델이 이해할 수 있는 시스템 수준 지시사항, 사용자 메시지, 어시스턴트 응답의 텍스트 표현으로 변환하는 방법을 설명하는 [Jinja2 코드](https://jinja.palletsprojects.com/en/stable/)를 포함합니다.

이 구조는 **상호작용 전반에 걸쳐 일관성을 유지하고 모델이 다양한 유형의 입력에 적절하게 응답하도록 보장**하는 데 도움을 줍니다.

다음은 `SmolLM2-135M-Instruct` 채팅 템플릿의 단순화된 버전입니다:

```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
당신은 Hugging Face가 훈련한 SmolLM이라는 도움이 되는 AI 어시스턴트입니다
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```
보시다시피, chat_template은 메시지 리스트가 어떻게 형식화될지를 설명합니다.

다음과 같은 메시지가 주어졌을 때:

```python
messages = [
    {"role": "system", "content": "당신은 기술적인 주제에 중점을 둔 도움이 되는 어시스턴트입니다."},
    {"role": "user", "content": "채팅 템플릿이 무엇인지 설명해주실 수 있나요?"},
    {"role": "assistant", "content": "채팅 템플릿은 사용자와 AI 모델 간의 대화를 구조화합니다..."},
    {"role": "user", "content": "어떻게 사용하나요?"},
]
```

이전 채팅 템플릿은 다음과 같은 문자열을 생성할 것입니다:

```sh
<|im_start|>system
당신은 기술적인 주제에 중점을 둔 도움이 되는 어시스턴트입니다.<|im_end|>
<|im_start|>user
채팅 템플릿이 무엇인지 설명해주실 수 있나요?<|im_end|>
<|im_start|>assistant
채팅 템플릿은 사용자와 AI 모델 간의 대화를 구조화합니다...<|im_end|>
<|im_start|>user
어떻게 사용하나요?<|im_end|>
```

`transformers` 라이브러리는 토크나이저 과정의 일부로 채팅 템플릿을 처리합니다. transformers가 채팅 템플릿을 어떻게 사용하는지에 대해 <a href="https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates" target="_blank">여기</a>에서 더 자세히 읽어보세요. 우리가 해야 할 일은 메시지를 올바른 방식으로 구조화하는 것뿐이며, 토크나이저가 나머지를 처리할 것입니다.

다음 Space에서 동일한 대화가 서로 다른 모델의 해당 채팅 템플릿을 사용하여 어떻게 형식화되는지 실험해볼 수 있습니다:

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

### 메시지에서 프롬프트로

LLM이 대화를 올바르게 형식화된 상태로 받도록 하는 가장 쉬운 방법은 모델의 토크나이저에서 `chat_template`을 사용하는 것입니다.

```python
messages = [
    {"role": "system", "content": "당신은 다양한 도구에 접근할 수 있는 AI 어시스턴트입니다."},
    {"role": "user", "content": "안녕하세요!"},
    {"role": "assistant", "content": "안녕하세요, 어떤 도움이 필요하신가요?"},
]
```

이전 대화를 프롬프트로 변환하려면, 토크나이저를 로드하고 `apply_chat_template`을 호출합니다:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

이 함수가 반환하는 `rendered_prompt`는 이제 선택한 모델의 입력으로 사용할 준비가 되었습니다!

> 이 `apply_chat_template()` 함수는 ChatML 형식의 메시지와 상호작용할 때 API의 백엔드에서 사용될 것입니다.

이제 LLM이 채팅 템플릿을 통해 입력을 어떻게 구조화하는지 살펴보았으니, 에이전트가 환경에서 어떻게 행동하는지 알아보겠습니다.

에이전트가 이를 수행하는 주요 방법 중 하나는 도구를 사용하는 것입니다. 이는 AI 모델의 기능을 텍스트 생성을 넘어 확장합니다.

향후 유닛에서 메시지에 대해 다시 논의할 것입니다. 하지만 지금 더 자세히 알아보고 싶다면 다음을 확인해보세요:

- <a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank">Hugging Face 채팅 템플릿 가이드</a>
- <a href="https://huggingface.co/docs/transformers" target="_blank">Transformers 문서</a>