# AI 에이전트 관찰 가능성 및 평가

## 🔎 관찰 가능성이란 무엇인가요?

관찰 가능성은 로그, 메트릭, 추적과 같은 외부 신호를 통해 AI 에이전트 내부에서 일어나는 일을 이해하는 것입니다. AI 에이전트의 경우, 이는 액션, 도구 사용, 모델 호출, 응답을 추적하여 에이전트 성능을 디버깅하고 개선하는 것을 의미합니다.

![관찰 가능성 대시보드](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## 🔭 에이전트 관찰 가능성이 중요한 이유

관찰 가능성 없이는 AI 에이전트는 "블랙박스"입니다. 관찰 가능성 도구는 에이전트를 투명하게 만들어 다음과 같은 것을 가능하게 합니다:

- 비용과 정확도의 트레이드오프 이해
- 지연 시간 측정
- 유해한 언어 및 프롬프트 주입 감지
- 사용자 피드백 모니터링

다시 말해, 데모 에이전트를 프로덕션 환경에 준비시키는 것입니다!

## 🔨 관찰 가능성 도구

AI 에이전트를 위한 일반적인 관찰 가능성 도구로는 [Langfuse](https://langfuse.com)와 [Arize](https://www.arize.com)와 같은 플랫폼이 있습니다. 이러한 도구들은 상세한 추적을 수집하고 실시간으로 메트릭을 모니터링할 수 있는 대시보드를 제공하여 문제를 쉽게 감지하고 성능을 최적화할 수 있게 합니다.

관찰 가능성 도구는 기능과 능력이 매우 다양합니다. 일부 도구는 오픈소스이며, 로드맵을 형성하고 광범위한 통합을 제공하는 대규모 커뮤니티의 혜택을 받습니다. 또한, 특정 도구들은 LLMOps의 특정 측면(예: 관찰 가능성, 평가, 프롬프트 관리)에 특화되어 있는 반면, 다른 도구들은 전체 LLMOps 워크플로우를 커버하도록 설계되어 있습니다. 여러분에게 잘 맞는 솔루션을 선택하기 위해 다양한 옵션의 문서를 살펴보는 것을 권장합니다.

[smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index)와 같은 많은 에이전트 프레임워크는 [OpenTelemetry](https://opentelemetry.io/docs/) 표준을 사용하여 관찰 가능성 도구에 메타데이터를 노출합니다. 이 외에도, 관찰 가능성 도구들은 LLM의 빠르게 변화하는 세계에서 더 많은 유연성을 허용하기 위해 커스텀 계측을 구축합니다. 사용 중인 도구의 문서를 확인하여 지원되는 기능을 확인해야 합니다.

## 🔬 추적과 스팬

관찰 가능성 도구들은 보통 에이전트 실행을 추적과 스팬으로 표현합니다.

- **추적**은 시작부터 끝까지의 완전한 에이전트 작업을 나타냅니다(사용자 쿼리 처리와 같은).
- **스팬**은 추적 내의 개별 단계입니다(언어 모델 호출이나 데이터 검색과 같은).

![Langfuse에서의 smolagent 추적 예시](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## 📊 모니터링해야 할 주요 메트릭

관찰 가능성 도구들이 모니터링하는 가장 일반적인 메트릭들 중 일부는 다음과 같습니다:

**지연 시간:** 에이전트가 얼마나 빠르게 응답하나요? 긴 대기 시간은 사용자 경험에 부정적인 영향을 미칩니다. 에이전트 실행을 추적하여 작업과 개별 단계의 지연 시간을 측정해야 합니다. 예를 들어, 모든 모델 호출에 20초가 걸리는 에이전트는 더 빠른 모델을 사용하거나 모델 호출을 병렬로 실행하여 가속화할 수 있습니다.

**비용:** 에이전트 실행당 비용은 얼마인가요? AI 에이전트는 토큰당 과금되는 LLM 호출이나 외부 API에 의존합니다. 빈번한 도구 사용이나 여러 프롬프트는 비용을 빠르게 증가시킬 수 있습니다. 예를 들어, 에이전트가 약간의 품질 향상을 위해 LLM을 5번 호출한다면, 비용이 정당한지 또는 호출 횟수를 줄이거나 더 저렴한 모델을 사용할 수 있는지 평가해야 합니다. 실시간 모니터링은 또한 예상치 못한 급증(예: 과도한 API 루프를 일으키는 버그)을 식별하는 데 도움이 될 수 있습니다.

**요청 오류:** 에이전트가 실패한 요청은 몇 개인가요? 여기에는 API 오류나 실패한 도구 호출이 포함될 수 있습니다. 프로덕션 환경에서 이러한 것들에 대해 에이전트를 더 견고하게 만들기 위해 폴백이나 재시도를 설정할 수 있습니다. 예를 들어, LLM 제공자 A가 다운되면 백업으로 LLM 제공자 B로 전환합니다.

**사용자 피드백:** 직접적인 사용자 평가를 구현하면 가치 있는 인사이트를 얻을 수 있습니다. 여기에는 명시적 평가(👍좋아요/👎싫어요, ⭐1-5 별점)나 텍스트 코멘트가 포함될 수 있습니다. 지속적인 부정적 피드백은 에이전트가 예상대로 작동하지 않는다는 신호이므로 경고를 받아야 합니다.

**암시적 사용자 피드백:** 사용자 행동은 명시적 평가 없이도 간접적인 피드백을 제공합니다. 여기에는 즉각적인 질문 재구성, 반복된 쿼리 또는 재시도 버튼 클릭이 포함될 수 있습니다. 예를 들어, 사용자들이 같은 질문을 반복해서 한다면, 이는 에이전트가 예상대로 작동하지 않는다는 신호입니다.

**정확도:** 에이전트가 얼마나 자주 올바르거나 바람직한 출력을 생성하나요? 정확도 정의는 다양합니다(예: 문제 해결 정확도, 정보 검색 정확도, 사용자 만족도). 첫 번째 단계는 에이전트에 대한 성공이 무엇인지 정의하는 것입니다. 자동화된 검사, 평가 점수, 또는 작업 완료 레이블을 통해 정확도를 추적할 수 있습니다. 예를 들어, 추적을 "성공" 또는 "실패"로 표시하는 것입니다.

**자동화된 평가 메트릭:** 자동화된 평가를 설정할 수도 있습니다. 예를 들어, LLM을 사용하여 에이전트의 출력이 도움이 되는지, 정확한지, 그렇지 않은지 점수를 매길 수 있습니다. 또한 에이전트의 다양한 측면을 점수화하는 데 도움이 되는 여러 오픈소스 라이브러리가 있습니다. 예를 들어, RAG 에이전트를 위한 [RAGAS](https://docs.ragas.io/)나 유해한 언어나 프롬프트 주입을 감지하기 위한 [LLM Guard](https://llm-guard.com/)가 있습니다.

실제로는 이러한 메트릭들의 조합이 AI 에이전트의 상태를 가장 잘 파악할 수 있습니다. 이 장의 [예제 노트북](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb)에서 이러한 메트릭들이 실제 예제에서 어떻게 보이는지 보여드리겠지만, 먼저 일반적인 평가 워크플로우가 어떻게 생겼는지 배우겠습니다.

## 👍 AI 에이전트 평가하기

관찰 가능성은 메트릭을 제공하지만, 평가는 AI 에이전트가 얼마나 잘 수행되고 있는지, 그리고 어떻게 개선될 수 있는지 결정하기 위해 그 데이터(및 테스트 수행)를 분석하는 과정입니다. 다시 말해, 추적과 메트릭을 얻은 후, 그것들을 어떻게 사용하여 에이전트를 판단하고 결정을 내릴까요?

정기적인 평가가 중요한 이유는 AI 에이전트가 종종 비결정적이고(업데이트나 모델 동작의 변화를 통해) 진화할 수 있기 때문입니다 – 평가 없이는 "스마트 에이전트"가 실제로 제대로 일을 하고 있는지 아니면 퇴보했는지 알 수 없습니다.

AI 에이전트에 대한 평가는 두 가지 범주로 나뉩니다: **온라인 평가**와 **오프라인 평가**입니다. 둘 다 가치가 있으며 서로를 보완합니다. 우리는 보통 오프라인 평가로 시작하는데, 이는 에이전트를 배포하기 전에 필요한 최소한의 단계이기 때문입니다.

### 🥷 오프라인 평가

![Langfuse의 데이터셋 항목](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

이는 일반적으로 테스트 데이터셋을 사용하여, 실시간 사용자 쿼리가 아닌 제어된 환경에서 에이전트를 평가하는 것을 포함합니다. 예상 출력이나 올바른 동작을 알고 있는 큐레이션된 데이터셋을 사용한 다음, 에이전트를 그 위에서 실행합니다.

예를 들어, 수학 단어 문제 에이전트를 구축했다면, 알려진 답이 있는 100개의 문제로 구성된 [테스트 데이터셋](https://huggingface.co/datasets/gsm8k)을 가질 수 있습니다. 오프라인 평가는 종종 개발 중에 수행되며(CI/CD 파이프라인의 일부가 될 수 있음) 개선 사항을 확인하거나 퇴보를 방지하기 위한 것입니다. 이점은 **반복 가능하고 정답이 있기 때문에 명확한 정확도 메트릭을 얻을 수 있다는** 것입니다. 또한 사용자 쿼리를 시뮬레이션하고 에이전트의 응답을 이상적인 답변과 비교하거나 위에서 설명한 자동화된 메트릭을 사용할 수 있습니다.

오프라인 평가의 주요 과제는 테스트 데이터셋이 포괄적이고 관련성을 유지하는지 확인하는 것입니다 – 에이전트가 고정된 테스트 세트에서는 잘 수행할 수 있지만 프로덕션에서는 매우 다른 쿼리를 만날 수 있습니다. 따라서 테스트 세트를 실제 시나리오를 반영하는 새로운 엣지 케이스와 예제로 업데이트해야 합니다. 작은 "스모크 테스트" 케이스와 더 큰 평가 세트의 조합이 유용합니다: 빠른 검사를 위한 작은 세트와 더 넓은 성능 메트릭을 위한 더 큰 세트입니다.

### 🔄 온라인 평가

이는 실제 사용 중인 프로덕션 환경에서 에이전트를 평가하는 것을 의미합니다. 온라인 평가는 실제 사용자 상호작용에서 에이전트의 성능을 모니터링하고 결과를 지속적으로 분석하는 것을 포함합니다.

예를 들어, 실시간 트래픽에서 성공률, 사용자 만족도 점수 또는 기타 메트릭을 추적할 수 있습니다. 온라인 평가의 장점은 **실험실 환경에서는 예상하지 못했을 수 있는 것들을 포착한다는** 것입니다 – 시간이 지남에 따른 모델 드리프트(입력 패턴이 변화함에 따라 에이전트의 효과가 저하되는 경우)를 관찰하고 테스트 데이터에 없었던 예상치 못한 쿼리나 상황을 발견할 수 있습니다. 이는 에이전트가 실제 환경에서 어떻게 동작하는지에 대한 진정한 그림을 제공합니다.

온라인 평가는 종종 앞서 논의한 암시적 및 명시적 사용자 피드백 수집을 포함하며, 가능하면 섀도우 테스트나 A/B 테스트(에이전트의 새 버전이 이전 버전과 비교하기 위해 병렬로 실행됨)를 실행할 수 있습니다. 과제는 실시간 상호작용에 대한 신뢰할 수 있는 레이블이나 점수를 얻는 것이 까다로울 수 있다는 것입니다 – 사용자 피드백이나 다운스트림 메트릭(예: 사용자가 결과를 클릭했는지)에 의존할 수 있습니다.

### 🤝 두 가지 방법 결합하기

실제로, 성공적인 AI 에이전트 평가는 **온라인**과 **오프라인** 방법을 혼합합니다. 정의된 작업에서 에이전트를 정량적으로 점수화하기 위해 정기적인 오프라인 벤치마크를 실행하고, 벤치마크가 놓친 것들을 잡기 위해 실시간 사용을 지속적으로 모니터링할 수 있습니다. 예를 들어, 오프라인 테스트는 코드 생성 에이전트가 알려진 문제 세트에서의 성공률이 개선되고 있는지 확인할 수 있는 반면, 온라인 모니터링은 사용자들이 에이전트가 어려워하는 새로운 카테고리의 질문을 하기 시작했다는 것을 알려줄 수 있습니다. 두 가지를 결합하면 더 견고한 그림을 얻을 수 있습니다.

사실, 많은 팀들이 다음과 같은 루프를 채택합니다: _오프라인 평가 → 새 에이전트 버전 배포 → 온라인 메트릭 모니터링 및 새로운 실패 예제 수집 → 해당 예제를 오프라인 테스트 세트에 추가 → 반복_. 이렇게 하면 평가는 지속적이고 계속 개선됩니다.

## 🧑‍💻 실제로 이것이 어떻게 작동하는지 살펴보겠습니다

다음 섹션에서는 관찰 가능성 도구를 사용하여 에이전트를 모니터링하고 평가하는 방법의 예시를 보여드리겠습니다. 